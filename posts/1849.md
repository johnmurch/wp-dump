---
title: "How To Analyze A Folder Only? Easy, With Partial Crawl!"
date: 2014-10-30T00:00:00
excerpt: "<p>How to Analyze a Folder Only? Easy, with Partial Crawl! 30th October 2014Annabelle Did you ever wonder how to crawl only a portion of your website? Playing with the crawl&#8217;s start URL and the Virtual Robots.txt option in Botify Analytics allows to do just that. Let&#8217;s look at situations where you might want to perform&hellip; </p>
<p><a class=\"moretag\" href=\"https://www.botify.com/blog/partial-crawl-ba\">Read the full article</a></p>"
slug: partial-crawl-ba
---

<header class="text-center">
<h1 class="font-internacional font-regular normal text-header-one leading-header-one text-typography-accent-2">How to Analyze a Folder Only? Easy, with Partial Crawl!</h1>
<div class="flex items-center justify-center my-3"><span class="mr-1 font-internacional font-regular normal text-base leading-none text-typography-primary-lighter">30th October 2014</span><img decoding="async" alt="Annabelle" class="rounded-full w-10 h-10" src="//images.ctfassets.net/tp56mevc46jo/2fCkDEsbiQSWGIkcWs40mG/e548033eda97a957ca690bdc814ed048/HS-PNG-100x100-Annabelle_Bouard.png"><span class="ml-1 font-internacional font-regular normal text-base leading-none text-typography-primary">Annabelle</span></div>
</header>
<p><span class="font-roboto font-regular normal text-base leading-none Markdown__Container"></span></p>
<p>Did you ever wonder how to crawl only a portion of your website? Playing with the crawl&#8217;s start URL and the Virtual Robots.txt option in Botify Analytics allows to do just that. Let&#8217;s look at situations where you might want to perform a partial crawl, and see how to do it.</p>
<h2 id="why-analyze-a-section-of-your-website">Why Analyze a Section of Your Website</h2>
<p>There are many situations where restricting the crawl to an area of your website this might be useful. You may want to analyze:</p>
<ul>
<li>A folder with a linguistic version of a website</li>
<li>Everything BUT a folder &#8211; for example, an e-commerce website with an editorial section you want to leave out</li>
<li>A subdomain only, which happens to have its top navigation in another subdomain</li>
<li>The website&#8217;s navigation only, excluding content pages. For instance, in an e-commerce website, the category tree and search pages, but not product pages, nor user ratings etc.</li>
</ul>
<h2 id="crawl-a-folder-only">Crawl a Folder Only</h2>
<p>Let&#8217;s take the example of website with several linguistic versions, each in a folder:<br />
<a href="http://www.mywebsite.com/english/">www.mywebsite.com/english/</a><br />
<a href="http://www.mywebsite.com/spanish/">www.mywebsite.com/spanish/</a></p>
<p>Let&#8217;s say we want to crawl the english version only. The crawler setup will be as follows:</p>
<p><img decoding="async" alt="" src="https://gm01botify.wpengine.com/wp-content/uploads/2020/01/20141030_051437_Partial-domain-startURL.png"></p>
<p>Add a <a href="https://www.botify.com/blog/virtual-robots-txt">Virtual Robots.txt</a> to the crawler configuration: click on &#8220;Add Virtual Robots.txt&#8221;, copy your website&#8217;s robots.txt file content, and add rules to restrict the crawl to the folder.</p>
<p><img decoding="async" alt="" src="https://gm01botify.wpengine.com/wp-content/uploads/2020/01/20141029_042340_Partial-virtual-robots-txt.png"></p>
<p>Easy, right?</p>
<h2 id="crawl-everything-but-a-folder">Crawl Everything BUT a Folder</h2>
<p>It&#8217;s even easier:<br />
Start the crawl normally from the home page, copy your robots.txt file in the virtual robots.txt and simply add a disallow rule for that folder.</p>
<h2 id="crawl-a-subdomain-only-when-the-start-url-is-elsewhere">Crawl a Subdomain Only, When the Start URL is Elsewhere</h2>
<p>In many cases, crawling only a subdomain is very straightforward. No need to use a Virtual Robots.txt. All we need to do is list that subdomain as the only allowed domain, and make sure that the start URL is in that subdomain.</p>
<p>But what if the subdomain doesn&#8217;t have a proper home page? It becomes trickier if the top navigation for the subdomain&#8217;s content is placed in another subdomain. Which is quite common, actually: imagine that there is a forum subdomain (forum.mywebsite.com), and that the forum home &#8211; and perhaps its top navigation with the forum main themes &#8211; is on the main subdomain (<a href="http://www.mywebsite.com">www.mywebsite.com</a>):</p>
<p><img decoding="async" alt="" src="https://gm01botify.wpengine.com/wp-content/uploads/2020/01/20141030_062304_Partial-structure.png"></p>
<p>In that case, we need to allow both subdomains, and carefully list all the restrictions in the virtual robots.txt to only allow the top navigation in the main domain.</p>
<p>If the forum navigation is placed in a /forum/ folder on <a href="http://www.mydomain.com">www.mydomain.com</a>, all we need to do is use the following Virtual Robots.txt:</p>
<p><img decoding="async" alt="" src="https://gm01botify.wpengine.com/wp-content/uploads/2020/01/20141029_071758_Partial-virtual-robots-txt2.png"></p>
<p>This is a simple example. If forum URLs on <a href="http://www.mywebsite.com">www.mywebsite.com</a> were not conveniently located in a folder, we would have to list more detailed rules to cover them all with &#8220;Allow:&#8221; lines.</p>
<p>Notice that we used a [<a href="http://www.mywebsite.com%5D">www.mywebsite.com]</a> header, which means that the rules will only be applied to that domain. For forum.mywebsite.com, the Botify crawler will use the robots.txt file found online. If we wanted to restrict the crawl on the forum subdomain as well (for instance, crawl only a folder), we would add another section with a [forum.mywebsite.com] header &#8211; in that case, don&#8217;t forget the User-agent line, as each section corresponds to a full, independant robots.txt.<br />
For more details on the Virtual Robots.txt option&#8217;s functionality, check out the <a href="https://www.botify.com/support/#virtual">FAQ page</a>.</p>
<h2 id="crawl-navigation-only">Crawl Navigation Only</h2>
<p>If an e-commerce website has a large number of products, crawling the whole website is not the best approach to analyze the navigation structure: you may reach a very large number of URLs before covering all of the navigation. It can be interesting to perform an unrestricted crawl, and stop a at certain depth, and perform a second crawl on navigation only, with no depth limitation.</p>
<p>If the website has the following levels of navigation:</p>
<p>1) Universe (URL pattern: contains &#8220;_u[identifier]&#8221;)<br />
2) Section (URL pattern: contains &#8220;_s[identifier]&#8221;)<br />
3) Category (URL pattern: contains &#8220;_c[identifier]&#8221;)<br />
4) Subcategory (URL pattern: contains &#8220;_m[identifier]&#8221;)</p>
<p>The Virtual Robots.txt that allows to crawl only navigation is as follows:</p>
<p><img decoding="async" alt="" src="https://gm01botify.wpengine.com/wp-content/uploads/2020/01/20141030_045637_Partial-virtual-robots-txt-nav.png"></p>
<p>That&#8217;s it! You&#8217;re all set to start the crawl!</p>
<p>As a general rule, it is possible to analyze any part of a website, as long as we are able to define:</p>
<ul>
<li>An entry point (or, if needed, up to 3 entry points)</li>
<li>URL patterns that define the only links that must be followed by the crawler to remain within the allowed area &#8211; or the full list of URL patterns that are not allowed.</li>
</ul>
<p>Are you facing a situation which is not covered by our examples, and wondering how to proceed? Let us know!</p>
<div class="tags leading-big border-t border-b border-brand-quaternary-lighter mt-4"><span class="mr-1 font-roboto font-regular normal text-base leading-none">Category:</span><span><a class="uppercase text-typography-accent-1" href="/platform">Product Features</a></span></div>
<footer class="flex justify-center my-5 mx-5">
<div class="mr-1 w-1/2 text-right">
<p><span class="font-internacional font-regular normal text-base leading-none text-typography-primary">Previous Article</span></p>
<p><a class="inline-block mt-2" href="/blog/crawler-impact-performance"><span class="font-roboto font-regular normal text-base leading-none text-typography-accent-4">Crawl Speed: How Many Pages/Second? 7 Points to Take Into Account</span></a></p>
</div>
<div class="ml-1 w-1/2">
<p><span class="font-internacional font-regular normal text-base leading-none text-typography-primary">Next Article</span></p>
<p><a class="inline-block mt-2" href="/blog/interview-with-philippe-yonnet"><span class="font-roboto font-regular normal text-base leading-none text-typography-accent-4">&#8221; SEO Consultants Should Focus on Operational Guidance and Support&#8221; Philippe Yonnet, CEO, Search Foresight</span></a></p>
</div>
</footer>
<div shortname="botify" title="How to Analyze a Folder Only? Easy, with Partial Crawl!" url="https://www.botify.com/blog/partial-crawl-BA">
<div id="disqus_thread_old"></div>
<p><a class="dsq-brlink" href="http://disqus.com">Blog comments powered by <span class="logo-disqus">Disqus</span>.</a></p>
</div>
